\documentclass[10pt,A4paper]{article}

\usepackage{acronym}
\usepackage{amsmath,amssymb}
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\usepackage{changepage}
\usepackage{cite}
\usepackage{nameref,hyperref}
\usepackage[right]{lineno}
\usepackage[nopatch=eqnum]{microtype}

\usepackage{xcolor}
\usepackage{multirow}
\usepackage{ctable} % for \specialrule command
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage[outputdir=out]{minted}
\setminted{autogobble=true,linenos=true}

\bibliographystyle{abbrv}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\lfoot{\today}

\newcommand{\etal}{{\textit{et al. }}}
\newcommand{\mbx}{\mathbf{x}}
\newcommand{\mbu}{\mathbf{u}}
\newcommand{\mbp}{\mathbf{p}}
\newcommand{\mby}{\mathbf{y}}
\newcommand{\mbd}{\mathbf{d}}
\providecommand{\keywords}[1]{\textbf{Keywords } #1}

% Create a new environment for code samples.
\newfloat{code}{tbp}{lop}
\floatname{code}{Code Sample}

% Create acronyms to use in text
\newacro{ode}[ODE]{Ordinary Differential Equation}
\newacro{oed}[OED]{optimal experimental design}
\newacro{fim}[FIM]{Fisher information matrix}
\newacro{de}[DE]{Differential evolution}

\begin{document}
% ########################################################################
% ########################################################################
\vspace*{0.2in}
% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Experimental design for predictive models in microbiology depending on environmental variables}}
\newline
\\
Polina Gaindrik\textsuperscript{1,2},
Jonas Pleyer\textsuperscript{1,2},
Daniel Heger\textsuperscript{3},
Christian Fleck\textsuperscript{1,2}
\\
\bigskip
\textbf{1} \href{https://www.fdm.uni-freiburg.de/spatsysbio}{University of Freiburg}\\
\textbf{2} \href{https://www.fdm.uni-freiburg.de/spatsysbio}{Freiburg Center for Data Analysis and Modeling}\\
\textbf{3} \href{https://tsenso.com/en/}{tsenso GmbH}\\
\bigskip

\end{flushleft}
% ########################################################################
% ########################################################################
\section*{Abstract}
\linenumbers
The aim of predictive microbiology is to provide tools and methods for predicting the growth, survival, and death of microorganisms in different food matrices under a range of environmental conditions.
The parametrised mathematical models need to be calibrated using dedicated experimental data.
To efficiently plan experiments model-based experimental design is used.
In this chapter, we explain model-based experimental and provide step-by-step instructions for finding the optimal design using the well-known Baranyi\&Roberts growth model as an example.
To make this chapter self-consistent we provide a Python software {\it eDPM} for \ac{ode} -based models.\\
\keywords{Optimal experimental design, Parameter estimation, Fisher Information matrix, Identifiability, Uncertainty}

%
%
%
% ########################################################################
% ########################################################################
\section*{Introduction}
% \begin{figure}[h]
% 	\inputminted[linenos,firstline=57,lastline=79]{python}{../model-design-fischer-information-matrix/pool_model.py}
% 	\caption{Sample code written in Python~\cite{rossumPythonLanguageReference2010}.}
% \end{figure}
%
Mathematical modelling is a widely used tool to describe, understand and predict further behaviour of living systems.
In particular, in the field of Predictive Biology, one can find a large variety of works that dwell on building models of different levels of complexity controlled by model parameters, e.g., to describe bacteria growth \cite{bernaertsConceptsToolsPredictive2004}.
Predictive microbiology is a subfield of food microbiology that uses mathematical models to predict the behaviour of microorganisms in food.
It is based on the premise that the behaviour of microorganisms can be mathematically described by the use of mathematical models, which can take into account the effects of various factors such as temperature, pH, water activity, and the presence of preservatives, on the growth and survival of microorganisms.
These models can then be used to predict the behaviour of microorganisms under different conditions, and to evaluate the effectiveness of various control measures, such as refrigeration, heat treatment, or the addition of preservatives.
Predictive microbiology has many applications in food safety, as it can be used to assess the risk of foodborne illness, to design safe food processing and storage practices, and to develop new food products with extended shelf-life.
For successful predictions it is essential that the model parameters can be estimated from experimental data.
Due to measurement noise, the parameters can only be estimated with some uncertainties.
That gives rise to a set of important questions: given the model structure can all parameters be estimated?
What should be measured and when? What are the confidence intervals for the parameters? How can the experimental effort be minimised?
Answering these questions leads to finding the \ac{oed} where optimised experimental conditions and/or measurement times allow for a reduction of the experimental load without loss of information \cite{derlindenImpactExperimentDesign2013, balsa-cantoe.bangaj.r.COMPUTINGOPTIMALDYNAMIC2008}.
In general, the Experimental Design procedure can be used not only for the parameter estimation but also for model discrimination \cite{kreutzSystemsBiology2009, stamatiOptimalExperimentalDesign2016}.
However, in this chapter, we focus on Experimental Design for parameter estimation.
Comprehensive reviews can be found here: \cite{atkinsonDevelopmentsDesignExperiments1982, franceschiniModelbasedDesignExperiments2008,sunParameterEstimation2011,vilasPredictiveFood2016}.
Depending on the goal, a researcher faces an important choice, which of the several sometimes contradicting objectives should be chosen for a particular case.
For example, is it more desirable to have precise knowledge in a chosen set of parameters and less information about the remaining ones? What is the best balance between experimental effort and precision?
Using multi-objective approaches, some attempts were made to answer these questions and to improve the experimental design by combining several objectives \cite{telenOptimalExperimentDesign2012, logistRobustMultiobjectiveOptimal2011}.
For models which depend on external cues like temperature, it is {\it a priori} not clear whether constant are variable conditions are more efficient for the parameter estimation \cite{versyckIntroducingOptimal1999,garciaQualityShelflifePrediction2015}.
The whole parameter estimation process starts with choosing a model structure. Once this is chosen a first parameter set is selected based on the literature review or prior data from previous experiments.
Using this parameter set a first optimal experimental design is determined taking into account specific constraints, such as maximum number of measurements, measurement times, etc. Next, the designed experiments need to be performed and this data result into a new estimation of the parameters.
If the confidence intervals of the parameters are sufficiently small, the scheme ends, otherwise one uses the new estimates as the starting point for the next experimental design (see Fig. \ref{fig:expdesign_scheme}).
The process can be repeated several times to increase the precision of the parameter estimates until the desired accuracy is achieved.
To test the scheme one can also perform numerical experiments and obtain {\it in-silico} data.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{Figures/scheme.png}
    \caption{{\footnotesize The workflow of the iterative process for model-based experimental design for parameter estimation.}}
    \label{fig:expdesign_scheme}
\end{figure}

%
%
%
% ########################################################################
% ########################################################################
\section*{Materials and Methods}
There are several software packages available for model-based experimental design~\cite{balsa-canto_amigo2_2016, zhang_optimal_2018, busetto_near-optimal_2013}.
These packages comprise a large number of tools and due to this require some time to understand how to use them. to make this tutorial self-contained, we developed \mintinline[bgcolor=white,style=emacs]{bash}{eDPM} (Experimental Design for Predictive Microbiology), which is a set of tools to calculate the Sensitivity and Fischer Information Matrix and do parameter space exploration in order to find optimal results.
We expect a working installation of the popular scripting language Python $\geq3.7$~\cite{rossumPythonLanguageReference2010}.
For installation instructions, please refer to the website \href{https://www.python.org/downloads/}{python.org}.
We also expect users, to be able to write, execute and display output of scripts.
This tutorial can also be followed using jupyter notebooks \cite{jupyterteamJupyterNotebook}.
Users can obtain it by installing \mintinline[bgcolor=white,style=emacs]{bash}{eDPM} from \href{https://pypi.org/project/FisInMa/0.0.1/}{pypi.org}.
The python website has guides for installing packages \href{https://packaging.python.org/en/latest/tutorials/installing-packages/}{packaging.python.org/}.
Most Unix-Based systems (e.g. GNU/Linux) and Mac-OS can use \mintinline[bgcolor=white,style=emacs]{bash}{pip}
\begin{minted}[bgcolor=white]{bash}
pip install eDPM
\end{minted}
or \mintinline[bgcolor=white,style=emacs]{bash}{conda} to install the desired package.
\begin{minted}[bgcolor=white]{bash}
conda install eDPM
\end{minted}
The \href{https://spatial-systems-biology-freiburg.github.io/Fishi/}{Documentation} of the package is continuously updated.
After the installation of the package is complete, we open a file with a text editor of choice and simply start writing code.
We begin by writing a so-called she-bang which is responsible to signalize that this file is to be executed with python.
Afterwards, we import the needed packages.
\begin{code}[h]
    \begin{minted}[firstnumber=1]{python}
    #!/usr/bin/env python3

    import numpy as np
    from eDPM import *
    \end{minted}
    \caption{Import statements to use eDPM}
    \label{code:import_statements}
    % TODO make this FisInMa into a link
\end{code}
This will serve as the starting point for our script.
In the following, we will append more code to it and utilise the methods developed in eDPM~\cite{edpm2023}.
% TODO add citation which points to github repository
If line numbers are missing, these should be filled by blank lines for better readability.
%
%
% ########################################################################
\subsection*{Model Formulation}
\subsubsection*{Theory}
As a starting point it is necessary to define the mathematical model.
We restrict our discussion to biological system which can be described by a system of \aclp{ode}:
\begin{equation}
    \begin{cases}
    \dot \mbx (t) = f(t, \mbx, \mbu, \mbp) \\
    \mbx (t_0) = \mbx_0
    \label{eq:ode_general}
    \end{cases}.
\end{equation}
Here $\mbx = (x_1, x_2, ..., x_n)$ is a vector of state variables of the system with initial condition $\mbx_0$, $t$ is the time, $\mbu$ is a vector of a externally controlled inputs into the system (e.g., supply of glucose or the external temperature) and $\mbp$ are the parameters of the system.
We assume that a subset of the parameters $\mbp$ is unknown and should be estimated from data.
Predominantly, the state variables cannot be directly observed, but rather the observables are functions of the state variables:
\begin{equation}
    \mby (t) = g(t, \mbx (t), \mbu, \mbp) + \epsilon (t),
    \label{eq:observ_general}
\end{equation}
where the function $g$ is the observation or output function, and $\epsilon$ is the measurement noise.
The observational noise is often assumed to be an independently distributed Gaussian noise with zero mean and variance $\sigma^2$: $\epsilon (t) \sim N(0, \sigma^2)$, $\forall t$.\\

To demonstrate how Experimental Design works, we use in this tutorial the widely employed mathematical model by Baranyi and Roberts model \cite{baranyiDynamicApproach1994}, which was devised to describe bacteria growth.
The model introduces two state variables $\mbx = (x_1, x_2)$, where $x_1(t)$ denotes the cell concentration of a bacterial population at the time $t$ and $x_2(t)$ defines a physiological state of the cells, the process of adjustment (lag-phase):
\begin{equation}
    \begin{cases}
        \dot x_1(t) = \frac{x_2(t)}{x_2(t) + 1} \mu^\text{max} \big(1 - \frac{x_1(t)}{x_1^\text{max}}\big) x(t)  = f_1 \\
        \dot x_2(t) = \mu^\text{max}  x_2(t) = f_2
    \end{cases}.
    \label{eq:ode_BaranyiRoberts}
\end{equation}
Here $\mu^\text{max}$ determines the maximum growth rate, and $x_1^\text{max}$ is the maximal bacteria concentration due to environmental constraints.
To account for the influence of the temperature on the activity of the model, we will use the 'square root' or Ratkowsky-type model for the maximum growth rate~\cite{ratkowsky_relationship_1982}
\begin{equation}
    \sqrt{\mu^\text{max}} = b (T - T_\text{min}),
    \label{eq:RatkowskyModel}
\end{equation} 
where $b$ is the regression coefficient, and $T_\text{min}$ is the minimum temperature at which the growth can occur.
As the observable we choose the bacteria count, i.e:
\begin{equation}
    y(t) = x_1(t)+\epsilon(t),
 \end{equation} 
a common choice in predictive microbiology.
By this and the equations (\ref{eq:ode_BaranyiRoberts}), (\ref{eq:RatkowskyModel}) the system is fully defined.
Here $x_1^\text{max}, b, T_\text{min}$ are the parameters that we estimate using observational data $y$ at measurement times $t_i$, and temperature $T$ is an input of the system.
Based on this model, we would like to optimize the choice of measurement times as well as temperatures (inputs) of the system to find the \acl{oed}.
%
% ########################################################################
\subsubsection*{Code}
In order to be able to solve the equations numerically, we first need first to define the \ac{ode} system in python.
As was explained in the previous sections, this system consists of the equation (\ref{eq:ode_general}) with initial values $t_0,x_0$.
Next, we need to define all numerical values present in the system.
We distinguish between time points $t_i$ at which the result of the \ac{ode} is evaluated, inputs $u$, which alter the behaviour of our system (for example temperature, humidity, etc.), parameters $p$, which are the quantities that we want to estimate and other arguments which might be needed to solve the \ac{ode}.
Table \ref{tab:fsm-variables} provides an overview of these quantities and gives corresponding names in the code.
In the following, we will step-by-step explain how to specify all variables needed by using the example of the Baranyi-Roberts Model (Eq. \ref{eq:ode_BaranyiRoberts}).
\begin{table}[H]
    \centering
    \begin{tabular}{ccccc}
        \specialrule{.1em}{.01em}{.05em}
        Description & Formula & Code \\[1ex]
        \toprule \vspace{1mm} 
        ODE & $f$                       & \mintinline[bgcolor=white,style=emacs]{python}{def ode_fun(t, x, u, p, ode_args):} \\
            & $\partial f / \partial x$ & \mintinline[bgcolor=white,style=emacs]{python}{def ode_dfdx(t, x, u, p, ode_args):} \\[0.5ex]
            & $\partial f / \partial p$ & \mintinline[bgcolor=white,style=emacs]{python}{def ode_dfdp(t, x, u, p, ode_args):} \\[0.5ex]
        \midrule
        Observable  & $g$                       &  \mintinline[bgcolor=white,style=emacs]{python}{def obs_fun(t, x, u, p, ode_args)}\\[0.5ex]
        (optional)  & $\partial g / \partial x$ &  \mintinline[bgcolor=white,style=emacs]{python}{def obs_dgdx(t, x, u, p, ode_args)}\\[0.5ex]
                    & $\partial g / \partial p$ &  \mintinline[bgcolor=white,style=emacs]{python}{def obs_dgdp(t, x, u, p, ode_args)}\\[0.5ex]
        \midrule
        Initial value   & $x_0$ & \mintinline[bgcolor=white,style=emacs]{python}{x0}\\[0.5ex]
        Initial time    & $t_0$ & \mintinline[bgcolor=white,style=emacs]{python}{t0}\\[0.5ex]
        Time points     & $t_i$ & \mintinline[bgcolor=white,style=emacs]{python}{t}\\[0.5ex]
        Inputs          & $u$   & \mintinline[bgcolor=white,style=emacs]{python}{u}\\[0.5ex]
        Parameters      & $p$   & \mintinline[bgcolor=white,style=emacs]{python}{p}\\[0.5ex]
        Other arguments &       & \mintinline[bgcolor=white,style=emacs]{python}{ode_args}\\[0.5ex]
        \bottomrule
    \end{tabular}
    \caption{Summary of user-defined functions and variables.}
\label{tab:fsm-variables}
\end{table}
\paragraph{Defining the \acs{ode}}
We must define a few functions in python.
The first function will be the right-hand side of equation (\ref{eq:ode_general}), i.e., we need to implement the Baranyi-Roberts model (\ref{eq:ode_BaranyiRoberts}) into a function. 
The implementation can be seen in Figure \ref{code:code_baranyi_roberts_ode_fun}.
We explain the individual steps to create this function:
\begin{itemize}
\item  \mintinline[bgcolor=white,style=emacs]{python}{def ode_fun(t, x, u, P, ode_args)}\\
The function name can be chosen freely, but the order of required function arguments is fixed by the definition.  
\item \mintinline[bgcolor=white,style=emacs]{python}{(x1, x2) = x}\\
\mintinline[bgcolor=white,style=emacs]{python}{(Temp,...) = u}\\
\mintinline[bgcolor=white,style=emacs]{python}{(x_max, b, Temp_min) = p}\\
Unpack the input vectors \mintinline[bgcolor=white,style=emacs]{python}{x,u,p} for easy access of their components
\item \mintinline[bgcolor=white,style=emacs]{python}{mu_max = b**2 * (Temp - Temp_min)**2}\\
Calculate the maximum growth rate.
\item \mintinline[bgcolor=white,style=emacs]{python}{return [}\\
\mintinline[bgcolor=white,style=emacs]{python}{mu_max * (x2/(x2 + 1)) * (1 - x1/x_max) * x1}\\
\mintinline[bgcolor=white,style=emacs]{python}{mu_max * x2}\\
\mintinline[bgcolor=white,style=emacs]{python}{]}\\
Calculate the right hand side of the \acs{ode}, store it in a list (\mintinline[bgcolor=white,style=emacs]{python}{[ ... ]}) and return it.
\end{itemize}

\begin{code}[h]
    \begin{minted}[firstnumber=6]{python}
    def baranyi_roberts_ode(t, x, u, p, ode_args):
        (x1, x2) = x
        (Temp, ) = u
        (x_max, b, Temp_min) = p
        # Define the maximum growth rate
        mu_max = b**2 * (Temp - Temp_min)**2
        return [
            mu_max * (x2/(x2 + 1)) * (1 - x1/x_max) * x1,
            mu_max * x2
        ]
    \end{minted}
    \caption{Definition of the Baranyi-Roberts \ac{ode} model.}
    \label{code:code_baranyi_roberts_ode_fun}
\end{code}
%
% ########################################################################
\subsection*{Parameter Estimation}
After defining the model, the user need to provide an initial parameter set.  
This could be chosen from the literature or estimated from previously gathered experimental data. 
It is common to assume that the observational or measurement noise is Gaussian white noise (e.g., no temporal correlations) with zero mean and variance $\sigma(t)^2$: $\epsilon(t) \sim N(0, \sigma(t)^2)$, $\forall t$. 
In this case the logarithm of the likelihood function (log-likelihood) is given by:
\begin{equation}
    \ln L(\mbp) \propto - \sum_{i}\frac{ \big(\mathbf{g}^{i}(\mbp) - \mbd^{i}\big)^2}{2 \sigma_{i}^2}.
    \label{eq:likelihood_Gaussian}
\end{equation}
We introduced the following short hand notations: 
\begin{itemize}
\item $\mbd^{i}$: data measured at time $t=t_i$
\item $\mathbf{g}^{i}(\mbp)$: the observation function depending on the parameter vector $\mbp$ evaluated at time $t=t_i$
\item $\sigma_{i}^2$: variance of the observational noise at time $t=t_i$.
\end{itemize}
The method of maximum likelihood consist in searching for the parameter vector $\mbp$ which maximizes the likelihood or equivalently minimizes the log-likelihood \cite{gaborRobustEfficient2015}. 

%
% ########################################################################
\subsection*{Experimental Design}
After defining the model and setting the initial parameter vector $\mbp_0$ we can proceed to the Experimental Design. 
In essence, Experimental Design comprises maximizing an objective function depending on the model, the initial parameter vector $\mbp_0$, external input $\mbu$ into the system, and a set of discrete observation times $t_i$ at which the potential measurements should be performed. 
The objective function needs to quantify the information the observation $\mby$ has about the parameter vector $\mbp$. 
A common choice here is constructing objective functions based on the \ac{fim} \cite{lyTutorialFisher2017}. 
According to the so-called 'Cramer-Rao inequality', the Fisher information is inversely proportional to the minimal squared estimation error \cite{friedenExploratoryData2010}. 
Due to this relationship maximization of the information leads to a minimization of the variance or uncertainty in the parameters. 
In the following we explain one of the ways to calculate the \acl{fim} for an \acp{ode} system (\ref{eq:ode_general}) and the observable function (\ref{eq:observ_general}) \cite{lyTutorialFisher2017}.

%
% ########################################################################
\subsubsection*{Sensitivity Calculation}
An important ingredient into the \ac{fim} are local sensitivities. 
Assume that functions $f$ and $g$ are differentiable functions with respect to the state variables $\mbx$ and parameters $\mbp$. 
The local sensitivities are defined by $s^x_{ij} = (\mathrm{d} x_i / \mathrm{d} p_j )$. 
These can be calculated using an enhanced system of the \acp{ode}:
\begin{equation}
    \begin{cases}
    \dot x_i (t) = f_i(t, \mbx, \mbu, \mbp)\\
    \dot s^x_{ij} = \sum_k \frac{\partial f_i}{\partial x_k} s^x_{kj} + \frac{\partial f_i}{\partial p_j}
    \end{cases}.
\label{eq:ode_and_sensitiv}
\end{equation} 
For the \ac{fim} we need the sensitivities of observables functions $(\mathrm{d} y_i / \mathrm{d} p_j) = s_{ij}$. 
We determine these at a certain times $t_m$ and input $u_n$ using the solutions of $s^x_{ij}$:
\begin{equation}
    s_{ij} (t_m, u_n) = \sum_k \frac{\partial g_i}{\partial x_k}\bigg|_{t_m, u_n} s_{kj}^x (t_m, u_n) + \frac{\partial g_i}{\partial p_j}\bigg|_{t_m, u_n}.
\label{eq:observ_sensitivities}
\end{equation}

In the case the parameters are of very different scale (e.g., on the second and on the day scale), it may be advisable to use the relative or normalised sensitivities to improve not the absolute but the relative accuracy of the parameter estimates.: 
\begin{equation}
    \tilde{s}_{ij} (t_m, u_n) =\frac{\mathrm{d}\ln(y_i)}{\mathrm{d}\ln(p_j)} = \frac{\mathrm{d} y_i}{\mathrm{d} p_j} \frac{p_j}{y_i}\bigg|_{t_m, u_n}.
\label{eq:relat_sensitivities}
\end{equation}
These sensitivities are the elements of the sensitivity matrix \cite{stigterObservabilityComplex2017}. 
For example, in case of two observables $\mby = (y_1, y_2)$, two different inputs $\mbu = (u_1, u_2)$, $N$ different measurement times and $N_p$ parameters, sensitivity matrix reads:

\begin{equation}
    S = 
\begin{pmatrix}
s_{11} (t_1, u_1) & ... & s_{1 N_p}(t_1, u_1) \\
\vdots  &   & \vdots  \\
s_{11} (t_{N}, u_1) & ... & s_{1 N_p} (t_{N}, u_1)\\
s_{11} (t_1, u_2) & ... & s_{1 N_p}(t_1, u_2) \\
\vdots  &   & \vdots  \\
s_{11} (t_N, u_2) & ... & s_{1 N_p} (t_N, u_2)\\

s_{21} (t_1, u_1) & ... & s_{2 N_p}(t_1, u_1) \\
\vdots  &   & \vdots  \\
s_{21} (t_{N}, u_1) & ... & s_{2 N_p} (t_{N}, u_1)\\
s_{21} (t_1, u_2) & ... & s_{2 N_p}(t_1, u_2) \\
\vdots  &   & \vdots  \\
s_{21} (t_N, u_2) & ... & s_{2 N_p} (t_N, u_2)
\end{pmatrix}
\label{eq:sens_matrix}
\end{equation}
This matrix we will use to directly calculate the \ac{fim} via equation:
\begin{equation}
    F = S^T Q^{-1} S,
\end{equation}
where $Q$ is the covariance matrix of measurement error~\cite{a_l_lloyd_sensitivity_2009, balsa-cantoComputationalProcedures2008}. 
If the measurements are independent, then only the diagonal elements of the matrix are non-zero: 
\begin{equation}
    Q = 
\begin{pmatrix}
    \sigma_{1}^2(t_1, u_1) & 0                      & 0                      & \dots  & 0                     \\
    0                      & \sigma_{1}^2(t_2, u_1) & 0                      & \dots  & 0                     \\
    0                      & 0                      & \sigma_{1}^2(t_1, u_2) & \dots  & 0                     \\
    \vdots                 & \vdots                 & \vdots                 & \ddots & \vdots                \\
    0                      & 0                      & 0                      & \dots  & \sigma_{2}^2(t_N, u_2)
\end{pmatrix}.
\label{eq:covar_matrix}
\end{equation}
Here $\sigma_{i}^2 (t_m, u_n)$ is the error of the measurements of observable $y_i$ at time point $t_m$ with input $u_n$.
The error can be either estimated from data or approximated by some error model. 
For example, one may consider that the error contribution consists of an absolute part that stays constant and a relative part that is proportional to observable value.
Then the diagonal elements of the covariance matrix take form:
\begin{equation}
\label{eq:error_model}
    \sigma_{i}^2 (t_m, u_n) = \gamma_\text{abs} + \gamma_\text{rel} \cdot y_i(t_m, u_n),
\end{equation}
where $\gamma_\text{abs}$ and $\gamma_\text{rel}$ are the coefficients determining the absolute and the relative error contribution, respectively. 
In case one uses relative sensitivities $\tilde{s}_{ij} (t_m, u_n)$, one needs to use relative measurements errors for the matrix $Q$:
\begin{equation}
    \tilde{\sigma}_{i}^2 (t_m, u_n) = \frac{\sigma_{i}^2 (t_m, u_n)}{y^2_i(t_m, u_n)}.
\end{equation} 
Note that this approach is heuristic and the matrix $F$ is not the Fisher information matri~\cite{versyckIntroducingOptimal1999, banks_generalized_2010}.
%
% ########################################################################
\subsection*{Numerical Sensitivity Calculation}
For the calculation of the sensitivities (Eq. \ref{eq:ode_and_sensitiv}) we need to supply the derivatives of $f$ with respect to the parameters and state variables. 
We define $\mbx=(x_1,x_2,x_3)$ and $\mbp=(p_1,p_2,p_3)$ with $p_1=x_{max}$, $p_2=b$ and $p_3=T_\text{min}$. 
Firstly, we calculate the mathematical derivatives $\partial f_i/\partial p_j$ and $\partial f_i/\partial x_j$ and then implement the corresponding functions. 
The first component of the Baranyi-Roberts model reads:
\begin{equation}
\label{eq:rob_bar_model}
    \dot x_1(t) = f_1(t, \mbx, \mbu, \mbp) = \mu^\text{max} \frac{x_2(t)}{x_2(t) + 1} \left(1 - \frac{x_1(t)}{x_1^\text{max}}\right) x_1(t),
\end{equation}
where $\sqrt{\mu^\text{max}}=b(T-T_\text{min})$. It follows:
\begin{alignat}{4}
    \frac{\partial\mu^\text{max}}{\partial p_1} &= 0\\
    \frac{\partial\mu^\text{max}}{\partial p_2} &= 2b(T-T_\text{min})^2 &=& \frac{2\mu^\text{max}}{b}\\
    \frac{\partial\mu^\text{max}}{\partial p_3} &= -2b^2(T-T_\text{min}) &=& \frac{2\mu^\text{max}}{T-T_\text{min}}
\end{alignat}
and consequently:
\begin{alignat}{7}
    \frac{\partial f_1}{\partial p_1}(t) &= - \mu^\text{max} &&\frac{x_2(t)}{x_2(t) + 1} &\frac{x_1(t)}{\left(x_1^\text{max}\right)^2} &&x_1(t)\\
    \frac{\partial f_1}{\partial p_2}(t) &= \frac{2\mu^\text{max}}{b} &&\frac{x_2(t)}{x_2(t) + 1} &\left(1 - \frac{x_1(t)}{x_1^\text{max}}\right) &&x_1(t)\\
    \frac{\partial f_1}{\partial p_3}(t) &= \frac{2\mu^\text{max}}{T-T_\text{min}} &&\frac{x_2(t)}{x_2(t) + 1} &\left(1 - \frac{x_1(t)}{x_1^\text{max}}\right) &&x_1(t).
\end{alignat}
Similarly, the derivatives $\partial f/\partial x_i$ are given by:
\begin{alignat}{7}
    % TODO
    \frac{\partial f_1}{\partial x_1}(t) &= \mu^\text{max} &\frac{x_2(t)}{x_2(t) + 1} && \left(-\frac{1}{x_1^\text{max}}\right) &&x_1(t)\\
    \frac{\partial f_1}{\partial x_2}(t) &= \mu^\text{max} &\left(\frac{1}{x_2(t) + 1} - \frac{x_2(t)}{(x_2(t) + 1)^2}\right) && \left(1 - \frac{x_1(t)}{x_1^\text{max}}\right) &&x_1(t).
    % TODO
\end{alignat}
The readers are encouraged to calculate the components $\partial f_2/\partial p_i$ and $\partial f_2/\partial x_i$ as an exercise. 
The resulting implemented functions in python can be seen in Code Sample~\ref{code:code_baranyi_roberts_ode_fun_derivatives}.
\begin{code}[H]
    \begin{minted}[firstnumber=20]{python}
    def ode_dfdp(t, x, u, p, ode_args):
        (x1, x2) = x
        (Temp, ) = u
        (x_max, b, Temp_min) = p
        mu_max = b**2 * (Temp - Temp_min)**2
        return [
            [
                mu_max * (x2/(x2 + 1)) * (x1/x_max)**2,
                2 * b * (Temp - Temp_min)**2 * (x2/(x2 + 1))
                    * (1 - x1/x_max)*x1,
                -2 * b**2 * (Temp - Temp_min) * (x2/(x2 + 1))
                    * (1 - x1/x_max)*x1
            ],
            [
                0,
                2 * b * (Temp - Temp_min)**2 * x2,
                -2 * b**2 * (Temp - Temp_min) * x2
            ]  
        ]

    def ode_dfdx(t, x, u, p, ode_args):
        (x1, x2) = x
        (Temp, ) = u
        (x_max, b, Temp_min) = p
        mu_max = b**2 * (Temp - Temp_min)**2
        return [
            [
                mu_max * (x2/(x2 + 1)) * (1 - 2*x1/x_max),
                mu_max * 1/(x2 + 1)**2 * (1 - x1/x_max)*x1
            ], 
            [
                0,
                mu_max
            ]
        ]
    \end{minted}
    \caption{Derivatives of the function $f$ of the Baranyi-Roberts model \ac{ode}.}
    \label{code:code_baranyi_roberts_ode_fun_derivatives}
\end{code}
%
% ########################################################################
\subsection*{Define numerical values}
After the definition of the structure of the \ac{ode}, we need to specify numerical values. 
It is good practice, to gather such definitions in the \mintinline[bgcolor=white,style=emacs]{python}{__main__} method of the python program as it was done in Figure~\ref{code:code_write_main_function_start}. 
We start by defining the parameters of the \ac{ode} (line 59) and afterwards the initial values (line 62). 
Next, we constrain the optimisation routine to find the best possible time points in the interval $t_i\in\left[t_\text{low},t_\text{high}\right]$. 
To do this, we write the times as a dictionary with entries \mintinline[bgcolor=white,style=emacs]{python}{{"lb":t_low, "ub":t_high, "n":n_times}} where \mintinline[bgcolor=white,style=emacs]{python}{n_times} is the number of discrete time points at which we want to sample the system (line 65). 
In contrast, if we wanted to specify explicit values for the sampling points, we would have needed to supply a list of time values or a \mintinline[bgcolor=white,style=emacs]{python}{np.ndarray} directly. 
One can see this approach  in line 70 of the code example. 
Here, we supply a \mintinline[bgcolor=white,style=emacs]{python}{np.ndarray} with explicit values, thus fixing the sampling points them for the optimisation routine.
\begin{code}[h]
    \begin{minted}[firstnumber=57]{python}
    if __name__ == "__main__":
        # Define parameters
        p = (np.exp(21.1), 0.038, 2)

        # Define initial conditions
        x0 = np.array([np.exp(2.36), 1 / (np.exp(2.66)-1)])

        # Define interval and number of sampling points for times
        times = {"lb":0.0, "ub":1500.0, "n":4}

        # Define explicit temperature points
        Temp_low = 4.0
        Temp_high = 8.0
        n_Temp = 3

        # Summarize all input definitions in list (only temperatures)
        inputs = [
            np.linspace(Temp_low, Temp_high, n_Temp)
        ]
    \end{minted}
    \caption{The main function contains the actual values for our model definition.}
    \label{code:code_write_main_function_start}
\end{code}
\subsubsection*{Defining Explicit Values and Sampling}
The difference between choosing explicit values and specifying a sampling range may be subtle at first glance, but in turn allows to very easily switch between fixed values and optimised sampling points. 
For example, suppose we want to optimise the temperature at which the experiments are performed, which means we let the optimisation algorithm pick the optimal temperature points such that the information gathered from the system is maximised. 
Then the difference between defining explicit values for the temperatures and defining an interval for optimisation can be understood in code sample~\ref{code:difference_explicit_sampling}.\newline
\begin{code}[h]
    \begin{minted}[linenos=false]{python}
    # This will choose values explicitly
    inputs = [
        np.linspace(Temp_low, Temp_high, n_Temp)
    ]
    # >>> inputs
    # [array([4., 6., 8.])]


    # This will sample in the interval
    inputs = [
        {"lb":Temp_low, "ub":Temp_high, "n":n_Temp}
    ]
    # >>> inputs
    # [{'lb': 4.0, 'ub': 8.0, 'n': 3}]


    \end{minted}
    \caption{Difference between choosing explicit values and sampling over a given interval.}
    \label{code:difference_explicit_sampling}
\end{code}%
The different variables can be treated individually as needed. 
Suppose, we have a system with temperatures and humidity as input variables.
We could decide to have the temperature optimised, but fix the humidity explicitly, because experimentally one can change the temperature continuously (or in discrete steps) but is restricted regarding the humidity. 
In our code, we simply would mix explicit and sampled definitions for individual inputs (see code sample~\ref{code:mix_sampling_explicit}). 
[MIR IST DIES NICHT GANZ KLAR, WIR MÜSSEN DARÜBER REDEN].
\begin{code}[h]
    \begin{minted}[linenos=false]{python}
        inputs = [
            # These values will be sampled
            {"lb":Temp_low, "ub":Temp_high, "n":n_Temp},
            # These are fixed
            np.array([0.4, 0.45, 0.5, 0.55])
        ]
    \end{minted}
    \caption{Mixing of explicit and sampling for inputs.}
    \label{code:mix_sampling_explicit}
\end{code}
%
% ########################################################################
\subsection*{Define Model}
After we have decided on the numerical values for our model, we need to put everything together. 
The \mintinline[bgcolor=white,style=emacs]{python}{FisherModel} class serves as the entry point. 
Here, we simply supply every previously made definition of variables and methods. 
When using the syntax as shown in code sample~\ref{code:model_definition}, the order of arguments does not matter. 
However, when only using \mintinline[bgcolor=white,style=emacs]{python}{FisherModel(x0, 0.0, ...)}, please pay attention to the order of arguments.
\begin{code}[H]
    \begin{minted}[autogobble=false,firstnumber=77]{python}
    # Create the FisherModel which serves as the entry point
    #  for the solving and optimization algorithms
    fsm = FisherModel(
        ode_x0=x0,
        ode_t0=0.0,
        ode_fun=baranyi_roberts_ode,
        ode_dfdx=ode_dfdx,
        ode_dfdp=ode_dfdp,
        ode_initial=x0,
        times=times,
        inputs=inputs,
        parameters=p
    )
    \end{minted}
    \caption{Define the full model.}
    \label{code:model_definition}
\end{code}
There are some optional arguments which can be set if desired ...\newline
[IST MIR NICHT GANZ KLAR. MÜSSEN DARÜBER REDEN].
% TODO
% TODO
For a full list of optional arguments, we refer to the \href{https://spatial-systems-biology-freiburg.github.io/FisInMa/}{documentation} of the package.
%
% ########################################################################
\subsubsection*{Identifiability}
Before proceeding with the optimisation, the reader need to check if the parameters of the system are identifiable, i.e, to examine if it is possible to obtain a unique solution for the parameters from the optimisation. 
It can happen that a subset of the parameters are non-identifiable. 
The non-identifiability can be due to the model structure or observables (structural non-identifiability) or insufficient data (practical non-identifiability) \cite{}. 
Structural non-identifiability should be avoided as it results in at least one parameter which could be freely chosen meaning there is no optimal value for this parameter. 
Fortunately, there is a quick and easy way to check whether the system is structural non-identifiable by calculating the rank of the sensitivity matrix \cite{}. 
For an identifiable system, the rank should coincide with the number of estimated parameters. 
Only if this condition is satisfied, we can continue with optimisation. 
In case the rank of the sensitivity matrix is less than the number of parameters it is necessary to change the structure of the model or change or increase the number of observables. 
Practical non-identifiability results in large confidence intervals \cite{} \newline

[STELLEN WIR DENN KEINE ROUTINE DAZU BEREIT?]
%
% ########################################################################
\subsubsection*{Optimality Criteria}
The Fisher Information matrix needs to be mapped on an objective function. There are several ways to do this resulting in different objectives also called criteria. 
Optimisation of the different objectives result in different experimental design, some of the most popular criteria are \cite{}:
\begin{itemize}
 \item \textbf{D-optimality criterion} maximises the determinant $\det (F)$ of the \ac{fim}. 
 Translated to the parameter space it means that the volume of the confidence region (see Fig. \ref{fig:opt_criteria}) (or the geometric mean of all errors) is minimised. 
 The confidence region at a confidence level of $\alpha$ [WHAT IS $\alpha$?] can be interpreted as there is a $\alpha \cdot 100 \%$ chance that the best parameter fit will belong to this area in case of repeating the experiment and estimations multiple times. 
 It is usually presented as a confidence ellipsoid. D-optimality is the most widely used criterion and is suitable even if the parameters have different dimensionalities [WHAT DOES THIS MEAN EXACTLY? THE OTHER CRITERIA ARE NOT SUITED IN THIS CASE? AND WHAT DOES DIMENSIONALITY MEAN?].    
\item \textbf{E-optimality criterion} maximises the smallest eigenvalue $\lambda_{\min}$ of the \ac{fim}, which is the same as reducing only the largest estimation error. 
\item \textbf{A-optimality criterion} maximises the sum of all eigenvalues $\sum_i \lambda_i$, which can be interpreted as minimising the algebraic mean of all errors.
 \item \textbf{Modified E-optimality criterion} maximises the ratio between the minimal and maximal eigenvalue $\lambda_{\min} / \lambda_{\max}$ [INTERPRETATION?].
\end{itemize}
Each of the criteria has its pros and cons so the reader should have a closer look at the various properties of these criteria, for instance, in Franceschini's and Macchietto's paper \cite{franceschiniModelbasedDesignExperiments2008}. 
For the geometrical interpretation of the criteria using the confidence region shown in Fig. \ref{fig:opt_criteria}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/optimality_criteria.pdf}
    \caption{{\footnotesize The confidence ellipsoid projection to the ($p_1$, $p_2$) parameter space. 
    The ellipsoid shows the geometrical meaning of the optimality criteria. 
    The center of the ellipsoid represents the estimated parameter values.
    The radii are the uncertainties of the estimates associated with different eigenvalues of the \ac{fim} $\lambda_1$, $\lambda_2$ ($\lambda_1 < \lambda_2$).
    The D-optimality aims to minimize the volume of the ellipsoid, the E-optimality minimizes the largest radius, A-optimality minimizes the perimeter of the rectangle that encloses the ellipse (dashed gray line), and, finally,  Modified E-optimality tends to make the ellipse as spherical as possible.}}
    \label{fig:opt_criteria}
\end{figure}
%
% ########################################################################
\subsubsection*{Optimization}
After defining the objective function based on the Fisher Information matrix the next step is to optimise the chosen optimality criteria. 
Finding the Experimental Design corresponds to finding the global maximum of the objective, which is a special case of optimal control problem \cite{}. 
This problem has been widely studied, and multiple numerical solution algorithms for both local and global optimisation were introduced \cite{}. 
In the supplied toolbox {\it eDPM}  [In the suggested toolbox DO YOU MEAN THE SOFTWARE YOU SUPPLY?], three methods were implemented: differential evolution, basin-hopping, and brute force [WHAT DO YOU MEAN BY THIS?]. 
Quite good results [WHAT ARE QUIZE GOOD RESULTS? FAST CONVERGENCE?] can be achieved by the \ac{de} algorithm developed by Storn and Price (1996) \cite{stornDifferentialEvolutionSimple1997}. 
It is one of the stochastic global optimisation methods appropriate for nonlinear dynamic problems. 
These algorithms have mild computational load but one cannot be sure that the absolute optimum is reached \cite{}. 
We give in the following a brief summary of the implemented methods. 
For more information the reader should turn to the available literature, e.g., \cite{}. 
\newline

For differential evolution an initial population of candidate vectors for the Experimental Design (sampling times and inputs) is randomly chosen from the region of available values. 
Then each vector mutates by mixing with other candidate vector. 
To a chosen vector from the initial population $D_0$, we add a weighted difference between two other randomly chosen vectors from the same set $(D_\text{rand1} - D_\text{rand2})$. 
This process is called mutation and a new vector $D_m$ is obtained. The next step is to construct a new trial solution.
This is done by choosing the elements of the trial vector either from the initial $D_0$ or the mutated $D_m$ vectors. 
For each new element of trial vector, a random number drawn uniformly from the interval [0, 1) and compared to the so-called recombination constant. 
If this random number is less than the recombination constant, then the trial vector element is chosen from the vector $D_m$, otherwise from $D_0$. 
The degree of mutation can be controlled by changing the recombination constant; the larger this constant is, the more often vector elements are chosen from $D_m$. 
Subsequently, the objective function is calculated using the trial vector and the result is compared to result obtained using the initial solution $D_0$; and the best of them is chosen for the next generation. 
This procedure is repeated for every solution candidate of the initial population, by means of which the new generation is build. 
The process of population mutation is repeated till the desired accuracy is achieved [UNKLAR; WAS IST DENN MIT "ACCURACY" GEMEINT?]. 
This method is rather simple and straightforward, and does not require any gradient calculation and is easy to parallelise \cite{}.
\newline

The second implemented method is basin-hopping developed by David Wales and Jonathan Doye \cite{walesGlobalOptimizationBasinHopping1997}. 
It combines the Monte-Carlo and local optimisation and works as follows. [DIE FOLGENDEN SÄTZE SIND LEIDER VÖLLIG UNVERSTÄNDLICH; BITTE NEU FORMULIEREN] The classic Monte-Carlo algorithm implies that the values of the optimised vector are perturbed and are either accepted or rejected. 
However, in this modified strategy, after perturbation, the vector is additionally subjected to local minimisation. And only after this procedure the move is accepted according to the Metropolis criterion.
\newline

And lastly, a simple brute force method was implemented as well. 
It is a grid search algorithm calculating the objective function value at each point of a multidimensional grid in a chosen region. 
The advantage of this algorithm is that we can be sure that the global minimum is achieved [DAS STIMMT NICHT; DAS GITTER-OPTIMUM WIRD GEFUNDEN] because all possibilities are checked. 
However, the downside of this technique is that rather slow and inefficient. 
Even though discretisation and reduction of the whole number of possible values [GRID SEARCH ALREADY IMPLIES DISCRETISATION; WAS IHR MEINT IST WOHL EIN GRÖBERES GITTER?] can improve the performance, the computational time of this method allows optimisation only for a small number of optimisation times or inputs. 
Some other methods for the optimal control problem the reader can find, for example, in the papers of Banga \etal \cite{BANGA2005407, bangaImprovingFoodProcessing2003}.
%
% ########################################################################
\subsubsection*{Optimisation Code}
The usage of three implemented optimisation methods is greatly simplified in our approach: 
\begin{minted}[linenos=false]{python}
fsr = find_optimal(fsm).
\end{minted}
[OHNE ARGUMENTE, WIE WERDEN DENN DIE OPTIMIERUNGSROUTINEN AUFGERUFEN? ICH NEHME AN MIT DEFAULT EINSTELLUNGEN?]
However, we retain their full functionality. Any optimisation argument of the aforementioned routines, can be specified: 
\begin{minted}[autogobble=false,firstnumber=98]{python}
    fsr = find_optimal(
        # Required argument: The model to optimise
        fsm,
        # Our custom options
        criterion=fisher_determinant,
        relative_sensitivities=True,
        # Options from scipy.optimise.differential_evolution
        recombination=0.7,
        mutation=(0.1, 0.8),
        workers=-1,
        popsize=10,
        polish=False,
    )
\end{minted}
A full list of optional arguments can be seen in the \href{https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization}{scipy documentation}~\cite{virtanenSciPyFundamentalAlgorithms2020}. 
In addition, there are some interesting optimisation options such as the optional arguments \mintinline[bgcolor=white,style=emacs]{python}{relative_sensitivities,criterion}, which are responsible for using relative sensitivities $\tfrac{dy}{dp}\tfrac{p}{y}$ and specifying the optimality criterion as explained in the previous section [WAS SIND DIE DEFAULT PARAMETER?. 
Please view the \href{https://spatial-systems-biology-freiburg.github.io/FisInMa/}{full documentation} for explanation.
The resulting class \mintinline[bgcolor=white,style=emacs]{python}{fsr} contains all definitions, current values and information on the optimisation process.
%
% ########################################################################
\subsubsection*{Plotting, Json, etc.}
Our package also provides the option to save results into Json file and automatically plot results for the ode solutions, observables and sensitivities:
\begin{minted}[autogobble=false,firstnumber=92]{python}
    plot_all_observables(fsr)
    plot_all_sensitivities(fsr)
    json_dump(fsr, "baranyi.json").
\end{minted}
%
%
%
% ########################################################################
% ########################################################################
\section*{Results}
As a summary of this tutorial, we would like to present the resulting output of our package. 
To this end we study the growth of a bacterial colony consisting of a single specie and describe it mathematically using the Baranyi and Roberts model given by Eqs. (\ref{eq:ode_BaranyiRoberts}),(\ref{eq:RatkowskyModel}). 
We would like to estimate the parameters of the model with as few experiments as possible to minimise the experimental effort. 
The observable is the bacterial count, i.e., the first component of the state variable vector $y = x_1$.
As an additional constraint we need to consider that the available climate chambers can only operate the temperature in the range from 3 to 12 degrees. 
For the starting values of the parameters we take from literature $x_1^\text{max}=10^8$, $b=0.2$, $T_\text{min}=1.0$, and the initial conditions of the \acp{ode} are chosen to be $x_1(0) = 10^3$, $x_2(0)=0.01$. 
Moreover, we choose for all the measurements points the error model given by Eq. (\ref{eq:error_model}) with $\gamma_\text{abs}=0.3$ and $\gamma_\text{rel}=0.1$. The task is to propose at which temperatures to perform the experiments and which sampling times to choose. 
The number of different temperatures of the experiments is not specified yet, but we aim to chose the lowest possible number. To solve this problem, we performed the Experimental Design using the D-optimality criterion, relative sensitivities, and the differential evolution optimisation method. 
To ensure the identifiability of the system, the rank of the sensitivity matrix should be equal to the number of parameters [WIE SOLL DER LESER DAS TESTEN?]. 
This condition is satisfied if at least two temperatures are measured and two time points for each temperature are measured, e.g., in total four experiments need to be performed [IST DAS WIRKLICH RICHTIG? DAS SCHEINT MIR EINE SEHR GERINGE ANZAHL AN EXPERIMENTEN ZU SEIN. BITTE MAL MIT DEN EXPERIMENTEN IN DER LITERATUR VERGLEICHEN, Z.B. HIER: 1. Gospavic, R., Kreyenschmidt, J., Bruckner, S., Popov, V. + Haque, N. Mathematical modelling for predicting the growth of Pseudomonas spp. in poultry under variable temperature conditions. International Journal of Food Microbiology 127, 290–297 (2008).] 
The resulting \ac{oed} for such a system is presented in Fig. \ref{fig:baranyi_roberts_observable}, \ref{fig:baranyi_roberts_sensitivities}.

\begin{figure}[H]
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_000_x_00.pdf}
      \end{subfigure}    
      \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_001_x_00.pdf}
      \end{subfigure}
    \caption{{\footnotesize The example of the output of the Experimental Design optimisation procedure for the Baranyi and Roberts model. 
    The line plot presents the model solution for the observable, and the scatter plot determines the time points chosen by Experimental Design. [DIE BILDER BITTE BESCHRIFTEN MIT A UND B, UND MEHR TEXT ALS ERKLÄRUNG. AUCH GIBT ES HIER KEINEN SCATTER PLOT, SONDERN FILLED CIRCLES ODER ÄHNLICH]}}
    \label{fig:baranyi_roberts_observable}
\end{figure}

As can be noticed, the algorithm has chosen two different timepoints for temperature 12.0 and  one time point for temperature value 5.9
Each of these time points corresponds to an extreme of at least one sensitivity (see Fig. \ref{fig:baranyi_roberts_sensitivities}). [WENN DAS STIMMT DANN SIND DOCH VIEL WENIGER EXPERIMENTE AUSREICHEND UM DIE PARAMETER ZU SCHÄTZEN ALS GEWÖHNLICH DURCHGEFÜHRT WERDEN. BITTE UNBEDINGT MIT DER LITERATUR VERGLEICHEN]
\begin{figure}[H]
    \begin{subfigure}{.9\textwidth}
      \centering
      \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_000_x_00_p_00.pdf}
    \end{subfigure}
    \begin{subfigure}{.9\textwidth}
      \centering
      \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_000_x_00_p_01.pdf}
    \end{subfigure}
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_000_x_00_p_02.pdf}
    \end{subfigure}
\end{figure}%
\begin{figure}[H]\ContinuedFloat
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_001_x_00_p_00.pdf}
      \end{subfigure}
      \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_001_x_00_p_01.pdf}
      \end{subfigure}
  
      \begin{subfigure}{.9\textwidth}
          \centering
          \includegraphics[scale=0.35]{Figures/Sensitivity_Results_baranyi_roberts_ode_fisher_determinant_rel_sensit_cont_2times_2temps_001_x_00_p_02.pdf}
      \end{subfigure}
    \caption{{\footnotesize The example of the sensitivities calculated for the Experimental Design optimisation procedure for the Baranyi and Roberts model.
    The line plots present the model solution for the sensitivities, and the scatter plots determine the time points chosen by Experimental Design. [DIE BILDER IN PANELS ZUSAMMENFASSEN UND BESCHRIFTEN]}} 
    \label{fig:baranyi_roberts_sensitivities}
    \end{figure}

Based on the results of the Experimental Design, the optimal number of measurement times is three, which may appear a very low number.ﬂ Indeed, consider how the determinant increases with the number of measurement times in the logarithmic scale (see Fig. \ref{fig:det_vs_ntimes}). It can be noticed that the slope of the curve is largest between point 1 and 2, in fact approximately fourteen orders of magnitude larger than the slope between the other points. For a higher number of times, informational profit stays at the same order of magnitude [WAS BEDEUTET DAS?]. As a result, the researcher can significantly reduce the experimental workload without a noticeable loss of information, which exemplifies the value of \ac{oed}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{Figures/det_vs_ntimes.pdf}
    \caption{{\footnotesize The determinant of the Fisher information matrix as a function of the number of measurement times for the Experimental Design consisted of two measured temperatures.}}
    \label{fig:det_vs_ntimes}
\end{figure}

\section*{Two-species resource competition}
As a second example we discuss a slightly more complicated system and extend the previous example of bacteria growth to a system where two species are present.
The species interact by competitive inhibition because they depend on a common nutrient resource. 
Analogously to the previous example, we denote the concentration of the two different species as $x_1$ and $y_1$, resp. 
The system can be described by the following set of \acp{ode}:
\begin{equation}
    \begin{cases}
        \dot x_1 = \alpha_x R x_1 \\
        \dot y_1 = \alpha_y R y_1 \\
        \dot R = -\frac{R}{n_\text{max}}(\alpha_x x_1+\alpha_y y_1)
    \end{cases}
    \label{eq:Lotka_Volterra_2species_1}
\end{equation}
where $\alpha_x, \alpha_y$ are the time and temperature dependent growth rates for population $x_1$, $y_1$, and $R$ represents the nutrient pool concentration, respectively. Using 
the conservation quantity $x_1 + y_1 + n_\text{max}R = n_\text{max}$ this system can be reduced to:
\begin{equation}
    \begin{cases}
        \dot x_1 = \alpha_x x_1\left(1-\frac{x_1+y_1}{n_\text{max}}\right) \\
        \dot y_1 = \alpha_y y_1\left(1-\frac{x_1+y_1}{n_\text{max}}\right).
    \end{cases}
    \label{eq:Lotka_Volterra_2species_2}
\end{equation}
For the growth rates we use \cite{baranyiDynamicApproach1994,?}
\begin{equation}
    \alpha_x (t, T) = b_x^2 (T - T_{\text{min}, x})^2 \frac{x_2(t)}{x_2(t) + 1}
    \label{eq:growth_rate_2species}
\end{equation}
Combining equations (\ref{eq:Lotka_Volterra_2species_2}, \ref{eq:growth_rate_2species}), we get the system of four differential equations:
\begin{equation}
    \begin{cases}
        \dot x_1 = b_x^2 (T - T_{\text{min}, x})^2 \frac{x_2}{x_2 + 1} x_1 (1 - \frac{x_1 + y_1}{n_\text{max}})\\
        \dot x_2 = b_x^2 (T - T_{\text{min}, x})^2 x_2 \\
        \dot y_1 = b_y^2 (T - T_{\text{min}, y})^2 \frac{y_2}{y_2 + 1} y_1 (1 - \frac{x_1 + y_1}{n_\text{max}})\\
        \dot y_2 = b_y^2 (T - T_{\text{min}, y})^2 y_2 \\
    \end{cases}
    \label{eq:model_2species_resource}
\end{equation}
where $x_2, y_2$ are the physiological states [WAS MEINT IHR DAMIT?] of the species $x_1$ and $y_1$, respectively. The parameter vector of this system reads: $\mbp = (n_\text{max}, b_x, T_{\text{min}, x}, b_y, T_{\text{min}, y})$. We use as initial values for two species $(x_{10}, x_{20}, y_{10}, y_{20}) = (10^5, 0.1, 10^3, 0.1)$ and the starting vector $\mbp_0= (10^8, 0.1, 1.0, 0.3, 1.0)$ to calculate the \ac{fim}. [WAS SIND GENAU DIE ZU OPTIMIERENDEN GRÖSSEN? BITTE HIER AUCH BESCHREIBEN.]

Other optimisation arguments were taken the same as in previous example. to get the new Optimal Experimental Design for shown in Fig. \ref{fig:baranyi_roberts_2species_observable}.
\begin{figure}[H]
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_2species_rel_sensit_cont_2times_2temps_000_x_00.pdf}
      \end{subfigure}    
      \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_2species_rel_sensit_cont_2times_2temps_000_x_01.pdf}
      \end{subfigure}
    \end{figure}%
    \begin{figure}[H]\ContinuedFloat      
      \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_2species_rel_sensit_cont_2times_2temps_001_x_00.pdf}
      \end{subfigure}      
      \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[scale=0.35]{Figures/Observable_Results_baranyi_roberts_ode_fisher_determinant_2species_rel_sensit_cont_2times_2temps_001_x_01.pdf}
      \end{subfigure}   
    \caption{{\footnotesize The Optimal Experimental Design for the Baranyi and Roberts model with two different species. 
    The line plot presents the model solution for the observable, and the scatter plot determines the time points chosen by Experimental Design. [SEHT MEINE KOMMENTARE BEI DEN ANDEREN BILDERN.]}}
    \label{fig:baranyi_roberts_2species_observable}
\end{figure}

The presented Design shows that the least needed number of experiments is two with temperatures of 4.6 and 12.0. 
Here in each experiment two observables were measured that correspond to the concentrations of two bacteria types. 
For the first temperature one time-point is needed while for the second one at least two measurement times were chosen.
With these experimental conditions the system is identifiable so the data is sufficient to estimate the parameters.

[WAS FEHLT IST DEN EFFEKT DES EXPERIMENTELLEN DESIGNS ZU ZEIGEN. DAZU MÜSST IHR FOLGENDES MACHEN: NEHMT EINEN PARAMETERVEKTOR ALS WAHR AN. STARTET MIT EINEM VEKTOR NICHT ZU WEIT WEG VON DIESEM. PRODUZIERT NUN IN SILICO DATEN UND SCHÄTZT DIE PARAMETER. ZEIGT DIE UNSICHERHEITEN AUF DEN PARAMETERN, AM BESTEN BENUTZT IHR DAZU DIE PROFIL-LIKELIHOOD.]
%
%
%
% ########################################################################
% ########################################################################
\section*{Supporting information}
%
%
%
\nolinenumbers
% ########################################################################
% ########################################################################
\bibliography{predictive-microbiology-software}

\end{document}
